{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Kernels for DNA Sequences (20 P)\n",
    "\n",
    "In this first part the weighted degree kernel (WDK) will be implemented for the purpose of classifying DNA sequences. We will use Scikit-Learn (http://scikit-learn.org/) for training SVM classifiers. The focus of this exercise is therefore on the computation of the kernels. The training and test data is available in the folder `splices-data`. The following code reads the DNA sequence data and stores it in numpy arrays of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "Xtrain = numpy.array([numpy.array(list(l.rstrip('\\r\\n'))) for l in open('splice-data/splice-train-data.txt','r')])\n",
    "Xtest  = numpy.array([numpy.array(list(l.rstrip('\\r\\n'))) for l in open('splice-data/splice-test-data.txt','r')])\n",
    "Ttrain = numpy.array([int(l) for l in open('splice-data/splice-train-label.txt','r')])\n",
    "Ttest  = numpy.array([int(l) for l in open('splice-data/splice-test-label.txt','r')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the weighted degree kernel described in the lecture. It applies to two genes sequences $x,z \\in \\{\\mathrm{A},\\mathrm{T},\\mathrm{G},\\mathrm{C}\\}^L$ and is defined as:\n",
    "\n",
    "$$\n",
    "k(x,z) = \\sum_{m=1}^M \\beta_m \\sum_{l=1}^{L-m+1} I(u_{l,m}(x) = u_{l,m}(z))\n",
    "$$\n",
    "\n",
    "where $l$ iterates over the whole genes sequence, $u_{l,m}(x)$ is a subsequence of $x$ starting at position $l$ and of length $m$, and $I(\\cdot)$ is an indicator function that returns $1$ it the argument is true and $0$ otherwise. We would like to implement a function that is capable of *efficiently* computing this weighted degree kernel for any degree $M$. For this, we will make use of the block method presented in the lecture.\n",
    "\n",
    "As a first step, we would like to implement a function `size2contrib`, which builds a mapping from a given block size to the kernel contribution, i.e. the sum of beta values associated to all substrings contained in this block. The relation between block size and contribution to the kernel score is as follows:\n",
    "\n",
    " * Block size 1: contribution =  $\\beta_1$\n",
    " * Block size 2: contribution =  $2 \\beta_1 + \\beta_2$\n",
    " * Block size 3: contribution =  $3 \\beta_1 + 2 \\beta_2 + \\beta_3$\n",
    " * etc.\n",
    "\n",
    "The function should return an integer array of size 101 containing the contribution of blocks of size zero, one, two, up to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def size2contrib(beta):\n",
    "\n",
    "    # extend beta to array of size 101\n",
    "    arr = np.zeros(101)\n",
    "    arr[:len(beta)] = beta\n",
    "    beta = arr\n",
    "    \n",
    "    # create beta weight matrix\n",
    "    beta_weights = np.zeros([101, 101])\n",
    "\n",
    "    # insert columns shifted by one row \n",
    "    col = np.arange(101)\n",
    "    for i in range(101):\n",
    "        beta_weights[:, i] = np.roll(col, shift=i)\n",
    "    beta_weights = np.tril(beta_weights)  # keep lower triangle\n",
    "    \n",
    "    s2ctable = beta_weights @ beta\n",
    "\n",
    "    return s2ctable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be tested on a simple weighted degree kernel of degree $3$ where beta coefficients are given by $\\beta_1=1,\\beta_2=3,\\beta_3=9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   5.,  18.,  31.,  44.,  57.,  70.,  83.,  96., 109.,\n",
       "       122., 135., 148., 161., 174., 187., 200., 213., 226.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size2contrib(numpy.array([1.0, 3.0, 9.0]))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented this index, we now focus on implementing the weighted degree kernel. Here, the function `wdk` we would like to implement receives two data arrays `X` and `Z`, and some parameter vector $\\beta$. The function should return the kernel Gram matrix associated to `X` and `Z`, and run *efficiently*, i.e. as much as possible performing operation over several data points simultaneously by means of array computations. *(Hint: An array of block sizes can be transformed to an array of kernel contributions by using the indexing operation `blockcontribs = s2ctable[blocksizes]`.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wdk(X, Z, beta):\n",
    "    \n",
    "    # initialize size and weights\n",
    "    samples_XZ = (len(X), len(Z))\n",
    "    weights = size2contrib(beta)\n",
    "    \n",
    "    blocksize = np.zeros([*samples_XZ], dtype=int)\n",
    "    K = np.zeros([*samples_XZ], dtype=float)\n",
    "    \n",
    "    # check similarity as boolean matrix\n",
    "    similarity_check = (X[:, None] == Z[None])\n",
    "    bool_arr = np.full([*samples_XZ, 1], fill_value=False, dtype=bool)\n",
    "    I = np.concatenate([similarity_check, bool_arr], axis=2)\n",
    "    \n",
    "    # loop over last axis and update kernel\n",
    "    for i in range(I.shape[-1]):\n",
    "        K += weights[blocksize] * (~I[..., i])\n",
    "        blocksize += I[..., i]\n",
    "        blocksize *= I[..., i]\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the weighted degree kernel has been implemented, the code below trains SVMs on the classification task of interest for different choices of parameters $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [1]                   training: 0.994  test: 0.916\n",
      "beta = [1 3 9]               training: 1.000  test: 0.963\n",
      "beta = [0 0 0 1 3 9]         training: 1.000  test: 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "for beta in [\n",
    "    numpy.array([1]),\n",
    "    numpy.array([1, 3, 9]),\n",
    "    numpy.array([0, 0, 0, 1, 3, 9]),\n",
    "]:\n",
    "    Ktrain = wdk(Xtrain, Xtrain, beta)\n",
    "    Ktest  = wdk(Xtest, Xtrain, beta)\n",
    "    mysvm = svm.SVC(kernel='precomputed').fit(Ktrain, Ttrain)\n",
    "    print('beta = %-20s  training: %.3f  test: %.3f'%(beta, mysvm.score(Ktrain, Ttrain), mysvm.score(Ktest, Ttest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that it is necessary to include non-unigram terms in the kernel computation to achieve good prediction performance. If however, we rely mainly on long substrings, e.g. 4-, 5-, and 6-grams, there are not sufficiently many matchings in the data to obtain reliable similarity scores, and the prediction performance decreases as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Kernels for Text (20 P)\n",
    "\n",
    "Structured kernels can also be used for classifying text data. In this exercise, we consider the classification of a subset of the 20-newsgroups data composed only of texts of classes `comp.graphics` and `sci.med`. The first class is assigned label `-1` and the second class is assigned label `+1`. Furthermore, the beginning and the end of the newsgroup messages are removed as they typically contain information that makes the classification problem trivial. Like for the genes sequences dataset, data files are composed of multiple rows, where each row corresponds to one example. The code below extracts the fifth message of the training set and displays its 500 first characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hat is, >>center and radius, exactly fitting those points?  I know how\n",
      "to do it >>for a circle (from 3 points), but do not immediately see a\n",
      ">>straightforward way to do it in 3-D.  I have checked some >>geometry\n",
      "books, Graphics Gems, and Farin, but am still at a loss? >>Please have\n",
      "mercy on me and provide the solution?   > >Wouldn't this require a\n",
      "hyper-sphere.  In 3-space, 4 points over specifies >a sphere as far as\n",
      "I can see.  Unless that is you can prove that a point >exists in\n",
      "3-space that  [...]\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "text = list(open('newsgroup-data/newsgroup-train-data.txt','r'))[4]\n",
    "print(textwrap.fill(text[:500]+' [...]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Texts to a Set-Of-Words\n",
    "\n",
    "A convenient way of representing text data is as \"set-of-words\": a set composed of all the words occuring in the document. For the purpose of this exercise, we formally define a word as an isolated sequence of at least three consecutive alphabetical characters. Furthermore, a set of `stopwords` containing mostly uninformative words such as prepositions or conjunctions that should be excluded from the set-of-words representation is provided in the file `stopwords.txt`. Create a function `text2sow(text)` that converts a text into a set of words following the just described specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sow(text):\n",
    "    \n",
    "    stopwords = [stopword[:-1] for stopword in open('stopwords.txt', 'r')]\n",
    "\n",
    "    list_of_words = []\n",
    "    for word in text.split():\n",
    "        \n",
    "        # filter out non-alphabetical characters\n",
    "        word_filter = filter(str.isalpha, word)\n",
    "        clean_word = \"\".join(word_filter)\n",
    "        \n",
    "        # filter out stopwords and words shorter than 3 characters\n",
    "        if not clean_word in stopwords and len(clean_word) > 2:\n",
    "            list_of_words.append(clean_word)\n",
    "\n",
    "    return set(list_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set-of-words implementation is then tested for the same text shown above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'center', 'surface', 'could', 'subject', 'specifies', 'exists',\n",
      "'close', 'Pictures', 'check', 'passing', 'way', 'may', 'Numerically',\n",
      "'perpendicular', 'right', 'define', 'must', 'find', 'loss', 'two',\n",
      "'Correct', 'one', 'But', 'wrong', 'Sorry', 'three', 'centre',\n",
      "'provide', 'diameter', 'relative', 'immediately', 'Consider',\n",
      "'straightforward', 'Call', 'normal', 'angles', 'lie', 'All', 'hat',\n",
      "'containing', 'circumference', 'require', 'know', 'circle', 'fitting',\n",
      "'ABC', 'Please', 'best', 'say', 'space', 'well', 'equidistant',\n",
      "'bisector', 'necessarily', 'Its', 'plane', 'coplaner', 'sphere',\n",
      "'see', 'take', 'books', 'quite', 'meet', 'infinity', 'Gems', 'mercy',\n",
      "'checked', 'desired', 'radius', 'either', 'The', 'Let', 'since',\n",
      "'least', 'exactly', 'rightangles', 'line', 'Algorithm',\n",
      "'noncollinear', 'need', 'Find', 'point', 'possibly', 'Farin',\n",
      "'normally', 'still', 'Graphics', 'failure', 'cannot', 'Check',\n",
      "'hypersphere', 'steve', 'four', 'fact', 'happen', 'This', 'choose',\n",
      "'Wouldnt', 'error', 'Yes', 'geometry', 'prove', 'defined',\n",
      "'bisectors', 'Any', 'coincident', 'far', 'Take', 'points',\n",
      "'intersection', 'Unless', 'lies', 'otherwise', 'solution'}\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(text2sow(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Set-Of-Words Kernel\n",
    "\n",
    "The set-of-words kernels between two documents $x$ and $z$ is defined as\n",
    "\n",
    "$$\n",
    "k(x,z) = \\sum_{w \\in \\mathcal{L}} I(w \\in x \\wedge w \\in z)\n",
    "$$\n",
    "\n",
    "where $I(w \\in x \\wedge w \\in z)$ is an indicator function testing membership of a word to both sets of words. As for the DNA classification exercise, it is important to implement the kernel in an efficient manner.\n",
    "\n",
    "The function `benchmark(text2sow,kernel)` in `utils.py` computes the worst-case performance (i.e. when applied to the two longest texts in the dataset) of a specific kernel implementation. Here, the function is tested on some naive implementation of the set-of-words kernel available in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel score: 741.000 , computation time: 0.692\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.benchmark(text2sow,utils.naivekernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to accelerate the procedure by sorting the words in the set-of-words in alphabetic order, and making use of the new sorted structure in the kernel implementation. In the code below, the sorted list associated to `sow1` is called `ssow1`. *Implement* a function `sortedkernel(ssow1,ssow2)` that takes as input two sets of words (sorted in alphabetic order) and that computes the kernel score in an efficient manner, by taking advantage of the sorting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortedkernel(ssow1, ssow2):\n",
    "    \n",
    "    intersection_set = set.intersection(set(ssow1), set(ssow2))\n",
    "    \n",
    "    k = len(intersection_set)\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This efficient implementation of the set-of-words kernel can be tested for worst case performance by running the code below. Here, we define an additional method `text2ssow(text)` for computing the sorted set-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel score: 741.000 , computation time: 0.000\n"
     ]
    }
   ],
   "source": [
    "def text2ssow(text): return sorted(list(text2sow(text)))\n",
    "\n",
    "import utils\n",
    "utils.benchmark(text2ssow, sortedkernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel score remains the same, showing that our new sorted implementation still produces the same function, however, the computation time has dropped drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Documents with a Kernel SVM\n",
    "\n",
    "The set-of-words kernel implemented above can be used to build a SVM-based text classifier. Here, we would like to separate our two classes `comp.graphics` and `sci.med`. The code below reads the whole dataset and stores it in a sorted set-of-words format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "Xtrain = list(map(text2ssow,open('newsgroup-data/newsgroup-train-data.txt','r')))\n",
    "Xtest  = list(map(text2ssow,open('newsgroup-data/newsgroup-test-data.txt','r')))\n",
    "Ttrain = numpy.array(list(map(int,open('newsgroup-data/newsgroup-train-label.txt','r'))))\n",
    "Ttest  = numpy.array(list(map(int,open('newsgroup-data/newsgroup-test-label.txt','r'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel matrices are then produced using our efficient kernel implementation with pre-sorting, and a SVM can be trained to predict the document class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1.000   test: 0.953\n"
     ]
    }
   ],
   "source": [
    "Ktrain = numpy.array([[sortedkernel(ssow1,ssow2) for ssow2 in Xtrain] for ssow1 in Xtrain])\n",
    "Ktest  = numpy.array([[sortedkernel(ssow1,ssow2) for ssow2 in Xtrain] for ssow1 in Xtest])\n",
    "mysvm = svm.SVC(kernel='precomputed').fit(Ktrain,Ttrain)\n",
    "print('training: %.3f   test: %.3f'% (mysvm.score(Ktrain,Ttrain),mysvm.score(Ktest,Ttest)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
